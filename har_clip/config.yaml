model:
  name: openai/clip-vit-base-patch32   # or openai/clip-vit-base-patch16

data:
  path: ./data/Structured

training:
  epochs: 10
  micro_batch_size: 64        # fits in VRAM
  accum_steps: 62             # 64 * 62 â‰ˆ 3968 (~4k effective)
  lr: 5e-5
  weight_decay: 0.02
  fp16: true
  dev_run: false
  dev_run_size: 256
  xbm_size: 4096              # ~4k negatives in the contrastive matrix

logging:
  out_dir: runs/clip
  print_every: 20
  seed: 1337
